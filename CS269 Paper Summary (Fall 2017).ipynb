{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<H2> Summary of : Adversarial Examples for Evaluating Reading Comprehension Systems</H2>\n",
    "CS269  Fall 2017, Paper Summary Assignment\n",
    "<br/>\n",
    "<hr />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper Title: ** Adversarial Examples for Evaluating Reading Comprehension Systems <br/ >\n",
    "**Authors: ** Robin Jia, Percy Liang (Stanford University) <br />\n",
    "**Published in:** EMNLP 2017 <br />\n",
    "**Paper link :** https://arxiv.org/abs/1707.07328\n",
    "\n",
    "Presented by: <br />\n",
    "**Name: ** [Moustafa Alzantot](http://cs.ucla.edu/~malzantot) &nbsp; &nbsp; ([malzantot@ucla.edu](mailto:malzantot@ucla.edu) ) <br/>\n",
    "**UID: ** 804432195 <br/>\n",
    "<hr/>\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, researchers have made a lot of progress in question answering and machine comprehension models. A machine comprehension model aimed to produce answer for a given question based on the understanding of a context document. Formally, it will compute $$ p(a| q, p)$$ \n",
    "where $a$ is the answer, $q$ is the question, and $p$ is the context paragraph.\n",
    "\n",
    "Evaluation of machine comprehension models based on standard datasets such as the Stanford Question Answering dataset (SQuaAD) have demonstrated promising results. However, if the model is really successful in understanding the given context document $d$ is should be able to still provide a right answer even when an adversarial tries to confuse it by adding extra senetence or word to the document.  \n",
    "In this paper, the authors present an experiment about adversarial evaluation for machine comprehension models. They show that despite state of art models can achieve near to human-level of accuracy (84.7% F1 score) in answering question from SQuAD dataset. These models are hugely affected by an adversary adding a **single confusing** statements that will degrade the models performance significantly. This implies, that models are not still really  good in comprehending the given document.\n",
    "\n",
    "<img src='images/img1.png'>\n",
    "\n",
    "In The example shown agove, the original prediction changes after adding the confusing statement (highlighted in  blue) by the adversary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Models\n",
    "\n",
    "The SQuaAD dataset is standard benchmark for machine comperhension models that consits of $107,785$ paragraphs from Wikipedia articles with question and answers about hte prograph contents.\n",
    "\n",
    "The authors of the paper picked two recent state-of-art deep learning models : BiDAF and Match-LSTM for their experiments to develop their adversarial attack. In addition to that, they also used other 12 published models for validation. The average F1 score of all question-answer pairs in the test set is used to evaluate the performance of model model. The F1 score of **robust model** should **not** be affected too much by the sentence that an adversary will add to the input document $d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adversarial Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike adversarial attacks on image recognition models where the attacker will *slightly* add small pertubation noise to every pixel in the input image. The author propose **concatinative adversary** which will concatenate an extra sentence to the input pragraph. The newly added adversarial sentence shouldnot contradict with the original answer of the question, yet it would cause the model  to produce a wrong answer. The new sentence can be added anywhere within the input paragraph, and the author have chosen to add at the end.\n",
    "\n",
    "The author propose two variations of concatinative attacks:\n",
    "\n",
    "- **AddSent Attack**: AddSent attack will generate a sentence that looks similar the question but doesnot contradict with the original right answer. The author porposed 4 steps approach to generate such as sentence (Shown in figure below). First step, will mutate the question to make sure it doesnot conflict with the right answer. Second and third steps will generate a fake answer and  plug it in  to convert the question into a statement.  Finally, fourth step will fix any grammer mistake with help of Amazon mechanical turk human workers. The added sentence will be appended to th end of the input paragraph.\n",
    "<img src='images/img2.png'/>\n",
    "\n",
    "- **AddAny Attack**: While the addSent attack will always add a grammatically correct sentence to the input paragraph. The addANY will a sequence of words that can be grammitcally incorrect sentence to confuse the machine comprehension model. The choice of words is being done by greedy search selection based on querying the victim model results after adding all possible choices of words. This attack is more powerful, but it requires many queries to victim model and the added sentence will look odd to a human reader due to its grammatical errors.\n",
    "<img src='images/img3.png'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors evaluate the effect of both attack methods **AddSENT** and **AddANY** against the state of art Match-LSTM and BiDAF machine comprehension models.\n",
    "\n",
    "#### Attack success \n",
    "As shown in Figure below , the F1 score of Both models goes down from 75-80% to around 30% by **AddSENT** Attack. Moreover, the **AddANY** attack is more powerful and will cause the F1-Score to go down till 5%.\n",
    "<img src=\"images/img4.png\">\n",
    "\n",
    "### Effect on human\n",
    "The authors also did a human experiment, to evaluate the impact of the adversarial sentence on human reading the text after adding it. The results (shown below) suggest humans were **not** sigifnicantlly affected by the new confousing sentence. This implies that the sentence does not conflict with the right answer.\n",
    "\n",
    "<img src=\"images/img5.png\"/>\n",
    "\n",
    "### Attack transferability\n",
    "\n",
    "The ahtuors evaluated the degree of attack transferability  between different models. By using other 12 state of art machine comprehension models for validation of attacks generated using the MatchLSTM and biDAF models. The result shuggest that the **AddSENT** attack is highly transferrable between different models as shown below.\n",
    "\n",
    "<img src=\"images/img6.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The authors propose two different mechanisms for adversarial attacks against machine comprehension (Question-answering) models. The two kinds of attack **AddSENT** and **AddANY** are both concatentative adversaries that append one more sentence to the end of input text. Although the added sentence doesnot have effect on a human reader, it will cause the machine learning model to produce wrong answer. This implies, that machine learning model are still unable to really comprehend the given text while answering the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
